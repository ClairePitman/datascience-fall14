{
 "metadata": {
  "name": "",
  "signature": "sha256:356587b89c52791770a3dc4d38089761a025018f48d9a09fa83c9d4285da79c0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 2: Information Extraction using NLTK\n",
      "\n",
      "[NLTK](http://www.nltk.org/) is a python platform for natural language processing and information extraction. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, and an active discussion forum.\n",
      "\n",
      "The associated book, [Natural Language Processing with Python](http://www.nltk.org/book/) is also freely available and is a great resource both for NLP concepts and for practical examples of how to use the NLTK package.\n",
      "\n",
      "To get started, you need to install NLTK:\n",
      "    \n",
      "    sudo pip install -U nltk\n",
      "    \n",
      "After running the first command below (download()), you will be presented with a window to select which 'data' to download. For space reasons, only download 'book' (Everything used in the NLTK book). This in itself is a few hundred MB download."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "showing info http://nltk.github.com/nltk_data/\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test it is working\n",
      "\n",
      "sentence = \"\"\"At eight o'clock on Thursday morning, Arthur didn't feel very good.\"\"\"\n",
      "tokens = nltk.word_tokenize(sentence)\n",
      "tagged = nltk.pos_tag(tokens)\n",
      "tagged"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "[('At', 'IN'),\n",
        " ('eight', 'CD'),\n",
        " (\"o'clock\", 'JJ'),\n",
        " ('on', 'IN'),\n",
        " ('Thursday', 'NNP'),\n",
        " ('morning', 'NN'),\n",
        " (',', ','),\n",
        " ('Arthur', 'NNP'),\n",
        " ('did', 'VBD'),\n",
        " (\"n't\", 'RB'),\n",
        " ('feel', 'VB'),\n",
        " ('very', 'RB'),\n",
        " ('good', 'JJ'),\n",
        " ('.', '.')]"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above two commands tokenize the string, and tag each of the token with the part-of-speech. Here is a listing of tagsets that NLTK uses here (there are different tagsets used by different corpora).\n",
      "http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following set of commands extracts named entities."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "entities = nltk.chunk.ne_chunk(tagged)\n",
      "entities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "Tree('S', [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'), (',', ','), Tree('PERSON', [('Arthur', 'NNP')]), ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')])"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Visual representation of the tree \n",
      "entities.draw()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Named Entity Recognition \n",
      "\n",
      "As a somewhat more elaborate example, the following sequence of commands reads data from a file, and does NER on each of the sentences in the file. It doesn't do a very good job on this article, but in general, it seems to work quite well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"news1.html\", \"r\") as myfile:\n",
      "    data = myfile.read()   \n",
      "sentences = nltk.sent_tokenize(data)\n",
      "sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
      "sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
      "print(nltk.ne_chunk(sentences[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  CDC/NNP\n",
        "  considers/NNS\n",
        "  adding/VBG\n",
        "  names/NNS\n",
        "  of/IN\n",
        "  health/NN\n",
        "  workers/NNS\n",
        "  monitored/VBD\n",
        "  for/IN\n",
        "  (PERSON Ebola/NNP)\n",
        "  to/TO\n",
        "  no-fly/NNP\n",
        "  list/NN\n",
        "  (PERSON Published/NNP)\n",
        "  October/NNP\n",
        "  16/CD\n",
        "  ,/,\n",
        "  2014/CD\n",
        "  FoxNews.com/JJ\n",
        "  </NN\n",
        "  http/NN\n",
        "  :/:\n",
        "  //www.foxnews.com//JJ\n",
        "  >/NN\n",
        "  (PERSON Facebook/NNP)\n",
        "  </NNP\n",
        "  #/#\n",
        "  >/:\n",
        "  558/CD\n",
        "  Twitter/NNP\n",
        "  </NNP\n",
        "  #/#\n",
        "  >/:\n",
        "  597/CD\n",
        "  livefyre/NN\n",
        "  </:\n",
        "  #/#\n",
        "  >/:\n",
        "  1623/CD\n",
        "  Email/NNP\n",
        "  </NNP\n",
        "  #/#\n",
        "  >/:\n",
        "  Print/NNP\n",
        "  </:\n",
        "  #/#\n",
        "  >/:\n",
        "  Now/NNP\n",
        "  Playing/NNP\n",
        "  CDC/NNP\n",
        "  :/:\n",
        "  Second/NNP\n",
        "  (PERSON Dallas/NNP Ebola/NNP)\n",
        "  patient/NN\n",
        "  took/VBD\n",
        "  commercial/JJ\n",
        "  flight/NN\n",
        "  Never/RB\n",
        "  autoplay/NN\n",
        "  videos/NNS\n",
        "  </:\n",
        "  #/#\n",
        "  >/:\n",
        "  The/DT\n",
        "  (ORGANIZATION Centers/NNPS)\n",
        "  for/IN\n",
        "  (PERSON Disease/NNP Control/NNP)\n",
        "  and/CC\n",
        "  Prevention/NNP\n",
        "  is/VBZ\n",
        "  considering/VBG\n",
        "  adding/VBG\n",
        "  the/DT\n",
        "  names/NNS\n",
        "  of/IN\n",
        "  healthcare/NN\n",
        "  workers/NNS\n",
        "  being/VBG\n",
        "  monitored/VBN\n",
        "  for/IN\n",
        "  the/DT\n",
        "  (ORGANIZATION Ebola/NNP)\n",
        "  virus/VBZ\n",
        "  to/TO\n",
        "  the/DT\n",
        "  government/NN\n",
        "  's/POS\n",
        "  no-fly/JJ\n",
        "  list/NN\n",
        "  ,/,\n",
        "  federal/JJ\n",
        "  officials/NNS\n",
        "  tell/VBP\n",
        "  (PERSON Fox/NNP News/NNP)\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Relation Extraction\n",
      "\n",
      "The second key task is to extract relations between entities. The following code snippet finds the relations between organizations and locations, in one of the existing datasets in NLTK. See the book webpage for more details on the regular expression pattern below: http://www.nltk.org/book/ch07.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
      "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
      "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern = IN):\n",
      "        print(nltk.sem.rtuple(rel))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ORG: u'WHYY'] u'in' [LOC: u'Philadelphia']\n",
        "[ORG: u'McGlashan &AMP; Sarrail'] u'firm in' [LOC: u'San Mateo']\n",
        "[ORG: u'Freedom Forum'] u'in' [LOC: u'Arlington']\n",
        "[ORG: u'Brookings Institution'] u', the research group in' [LOC: u'Washington']\n",
        "[ORG: u'Idealab'] u', a self-described business incubator based in' [LOC: u'Los Angeles']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[ORG: u'Open Text'] u', based in' [LOC: u'Waterloo']\n",
        "[ORG: u'WGBH'] u'in' [LOC: u'Boston']\n",
        "[ORG: u'Bastille Opera'] u'in' [LOC: u'Paris']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[ORG: u'Omnicom'] u'in' [LOC: u'New York']\n",
        "[ORG: u'DDB Needham'] u'in' [LOC: u'New York']\n",
        "[ORG: u'Kaplan Thaler Group'] u'in' [LOC: u'New York']\n",
        "[ORG: u'BBDO South'] u'in' [LOC: u'Atlanta']\n",
        "[ORG: u'Georgia-Pacific'] u'in' [LOC: u'Atlanta']\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Assignment \n",
      "\n",
      "**PART 2.1 (NER):** Download, say 10 recent news articles, on some topic. Write code to extract named entities from each of them. The final output should simply be a list of entities and their types, which would require understanding the structure of the output of the ne_chunk command, and traversing it to find just the named entities. \n",
      "\n",
      "Submit both the python code, and the entities you extracted. For example, for the article above, the output should be:\n",
      "\n",
      "    Ebola, PERSON\n",
      "    Published, PERSON\n",
      "    Facebook, PERSON\n",
      "    ...\n",
      "\n",
      "**Part 2.2 (Relation Extraction):** Write a few regular expressions to extract different types of PERSON-ORGANIZATION relationships (e.g., PERSON executive at ORGANIZATION) over the same dataset (the IEER Corpus). You can use the above script mostly unchanged with the changes being: definition of the pattern IN, and the arguments to `extract_rels`. \n",
      "\n",
      "It may be useful to see the text of some of the documents, e.g., the second document in the above corpus can be seen by running:\n",
      "\n",
      "    print(nltk.corpus.ieer.parsed_docs('NYT_19980315')[1].text)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"news2.html\", \"r\") as myfile:\n",
      "    data = myfile.read()   \n",
      "sentences = nltk.sent_tokenize(data)\n",
      "sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
      "sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
      "for sen in sentences:\n",
      "    \n",
      "    out=nltk.ne_chunk(sen)\n",
      "    for x in out:\n",
      "   \n",
      "     try:\n",
      "         x.label()\n",
      "     except AttributeError:\n",
      "          b = 0\n",
      "     else:\n",
      "         if x.label()=='PERSON':\n",
      "                    l = x.leaves() \n",
      "            \n",
      "                    if  len(l)>0: \n",
      "                        \n",
      "                        print 'PERSON, '+l[0][0]\n",
      "       \n",
      "            \n",
      "    \n",
      "#sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "PERSON, Ebola\n",
        "PERSON, Published\n",
        "PERSON, Facebook\n",
        "PERSON, Dallas\n",
        "PERSON, Disease\n",
        "PERSON, Fox\n",
        "PERSON, Dallas\n",
        "PERSON, Vinson\n",
        "PERSON, Thomas\n",
        "PERSON, Vinson\n",
        "PERSON, Vinson\n",
        "PERSON, Nina\n",
        "PERSON, Vinson\n",
        "PERSON, Vinson\n",
        "PERSON, David\n",
        "PERSON, Vinson\n",
        "PERSON, Ebola\n",
        "PERSON, Thomas\n",
        "PERSON, Vinson\n",
        "PERSON, Frieden\n",
        "PERSON, Vinson"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Frontier\n",
        "PERSON, Texas\n",
        "PERSON, Frieden\n",
        "PERSON, Anthony\n",
        "PERSON, Duncan\n",
        "PERSON, Ebola"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Duncan\n",
        "PERSON, Vinson\n",
        "PERSON, Obama\n",
        "PERSON, Obama"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Vinson\n",
        "PERSON, David"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Eric\n",
        "PERSON, Moines"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Iowa\n",
        "PERSON, Joni\n",
        "PERSON, Moines\n",
        "PERSON, Kentucky\n",
        "PERSON, Paul\n",
        "PERSON, Paul"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Ernst\n",
        "PERSON, Democrat\n",
        "PERSON, Ann\n",
        "PERSON, Harry\n",
        "PERSON, Reid\n",
        "PERSON, Campaign\n",
        "PERSON, Joni"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Mitch\n",
        "PERSON, Democrats\n",
        "PERSON, Louisiana\n",
        "PERSON, Barack"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Mitt\n",
        "PERSON, Fox\n",
        "PERSON, Michael"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Mitt\n",
        "PERSON, Debbie"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Obama\n",
        "PERSON, Bill\n",
        "PERSON, Clinton\n",
        "PERSON, Obama\n",
        "PERSON, Joe\n",
        "PERSON, Jersey\n",
        "PERSON, Chris\n",
        "PERSON, Paul\n",
        "PERSON, Texas\n",
        "PERSON, Priebus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Barack\n",
        "PERSON, Debbie\n",
        "PERSON, Harry\n",
        "PERSON, Iowa"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Alison\n",
        "PERSON, Party"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, David\n",
        "PERSON, Bill"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Rob\n",
        "PERSON, Louisiana"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Greg\n",
        "PERSON, Bill"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Mark\n",
        "PERSON, Clinton\n",
        "PERSON, Asa\n",
        "PERSON, Priebus\n",
        "PERSON, Hillary\n",
        "PERSON, Jeanne\n",
        "PERSON, Maggie\n",
        "PERSON, Tom\n",
        "PERSON, Michigan"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Gary\n",
        "PERSON, Dannel\n",
        "PERSON, Orman\n",
        "PERSON, Pat\n",
        "PERSON, Bob\n",
        "PERSON, Rick\n",
        "PERSON, Roberts\n",
        "PERSON, Jersey\n",
        "PERSON, Chris\n",
        "PERSON, Texas"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Mark\n",
        "PERSON, Charlie\n",
        "PERSON, Jeb\n",
        "PERSON, Rick\n",
        "PERSON, Dana\n",
        "PERSON, Bash\n",
        "PERSON, Will\n",
        "PERSON, Rachel"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Sat\n",
        "PERSON, Inside\n",
        "PERSON, Virgin\n",
        "PERSON, Richard\n",
        "PERSON, Virgin\n",
        "PERSON, Virgin\n",
        "PERSON, Virgin"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Virgin\n",
        "PERSON, Virgin\n",
        "PERSON, Richard\n",
        "PERSON, Virgin\n",
        "PERSON, Virgin"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Virgin\n",
        "PERSON, George"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Branson"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Virgin\n",
        "PERSON, Virgin\n",
        "PERSON, Branson\n",
        "PERSON, Inside"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Virgin\n",
        "PERSON, Bangladesh\n",
        "PERSON, Farid\n",
        "PERSON, Shops\n",
        "PERSON, Dhaka\n",
        "PERSON, Power"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Journalist"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Farid\n",
        "PERSON, Jethro\n",
        "PERSON, Sumarti"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Phillip"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Rappard\n",
        "PERSON, Katie\n",
        "PERSON, Thu\n",
        "PERSON, Music"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Amazon\n",
        "PERSON, Eric\n",
        "PERSON, Sullivan\n",
        "PERSON, Cheng"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Frank"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Cheng"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Cheng\n",
        "PERSON, Cheng"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Greg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Sat\n",
        "PERSON, Ahram"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Hisham"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Rotana\n",
        "PERSON, Ahram"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Ahram\n",
        "PERSON, Egypt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Salma\n",
        "PERSON, Chelsea"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Raja\n",
        "PERSON, Omar\n",
        "PERSON, Bani\n",
        "PERSON, Human\n",
        "PERSON, Victims"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Haq\n",
        "PERSON, Kataib\n",
        "PERSON, Joe"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Bani"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Asaib\n",
        "PERSON, Jomana"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Thu\n",
        "PERSON, Erbil"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Jana\n",
        "PERSON, Jana"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Meet\n",
        "PERSON, Jana\n",
        "PERSON, Nazand"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Begikhani\n",
        "PERSON, Begikhani\n",
        "PERSON, Narin"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Shamo\n",
        "PERSON, Arab"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Syrian\n",
        "PERSON, Begikhani\n",
        "PERSON, Arab"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "PERSON, Islam\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "IN = re.compile(r'(.*\\bof\\b(?!\\b.+ing))|(.*\\bfor\\b(?!\\b.+ing))|(.*\\bat\\b(?!\\b.+ing))|(.*\\bpresident of\\b(?!\\b.+ing))|(.*\\bexecutive at\\b(?!\\b.+ing))|(.*\\bpartner of\\b(?!\\b.+ing))|(.*\\bdirector of\\b(?!\\b.+ing))')\n",
      "\n",
      "\n",
      "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
      "    for rel in nltk.sem.extract_rels('PERSON', 'ORG', doc, corpus='ieer', pattern = IN):\n",
      "        print(nltk.sem.rtuple(rel))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[PER: u'Mike Godwin'] u', chief counsel for the' [ORG: u'Electronic Frontier Foundation']\n",
        "[PER: u'Robert Mergess'] u', the co-director of the' [ORG: u'Berkeley Center for Law and Technology']\n",
        "[PER: u'Jack Balkin'] u\", director of the school's program. ``What happened at\" [ORG: u'Yale']\n",
        "[PER: u'Michael Froomkin'] u'at the' [ORG: u'University of Miami']\n",
        "[PER: u'Dan Burk'] u'at' [ORG: u'Seton Hall University']\n",
        "[PER: u'David Post'] u', co-founder of the' [ORG: u'Cyberspace Law Institute']\n",
        "[PER: u'Frank Easterbrook'] u'of the' [ORG: u'7th U.S. Circuit Court of Appeals']\n",
        "[PER: u'Vern Fotheringham'] u', a founding partner of' [ORG: u'Qradio']\n",
        "[PER: u'William Gale'] u', an economist at the' [ORG: u'Brookings Institution']\n",
        "[PER: u'Joel Slemrod'] u', an economist at the' [ORG: u'University of Michigan']\n",
        "[PER: u'Alan Braverman'] u', Internet analyst at' [ORG: u'Credit Suisse First Boston']\n",
        "[PER: u'Bill Gross'] u', president of' [ORG: u'Idealab']\n",
        "[PER: u'Abe Kleinfield'] u', a vice president at' [ORG: u'Open Text']\n",
        "[PER: u'Kaufman'] u', president of the privately held' [ORG: u'TV Books LLC']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[PER: u'Kaufman'] u'would not disclose financial details of the deal.' [ORG: u'TV Books']\n",
        "[PER: u'Michael Coffey'] u', managing editor of' [ORG: u'Publishers Weekly']\n",
        "[PER: u'Lorne Michaels'] u\", the executive producer of ``Saturday Night Live.''\" [ORG: u'TV Books']\n",
        "[PER: u'James Billington'] u', the librarian of' [ORG: u'Congress']\n",
        "[PER: u'Sherry Lansing'] u', chairwoman of the' [ORG: u'Paramount Motion Picture Group']\n",
        "[PER: u'Lindsay Doran'] u', president of' [ORG: u'United Artists']\n",
        "[PER: u'Laura Ziskin'] u', president of' [ORG: u'Fox 2000']\n",
        "[PER: u'Rick Yorn'] u', his manager at the firm' [ORG: u'Addis-Wechsler &AMP; Associates']\n",
        "[PER: u'Tom Rothman'] u', president of production at' [ORG: u'20th Century Fox']\n",
        "[PER: u'Charlotte Forest'] u', executive producer for' [ORG: u'Homestead Editorial']\n",
        "[PER: u'Richard Strauss'] u\"' ``Salome'' at\" [ORG: u'La Scala']\n",
        "[PER: u'Philip Glass'] u'. This was not part of the' [ORG: u'Met']\n",
        "[PER: u'John Wren'] u', the president and chief executive at' [ORG: u'Omnicom']\n",
        "[PER: u'Charlie Moss'] u', the chairman of' [ORG: u'Moss/Dragoti']\n",
        "[PER: u'Ken Kaess'] u', president of the' [ORG: u'DDB Needham']\n",
        "[PER: u'Linda Kaplan Thaler'] u', who worked at' [ORG: u'Wells']\n",
        "[PER: u'Kaplan Thaler'] u'had worked for' [ORG: u\"Toys ``R'' Us\"]\n",
        "[PER: u'Ken Haldin'] u', a spokesman for' [ORG: u'Georgia-Pacific']\n"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}